# -*- coding: utf-8 -*-
"""Qwen-soap-finetune-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YdpBo_7gCNBdqPE6Fhk-ty6FN7XPNlQo

# Fine-tuning Qwen 2.5 for Medical SOAP Note Generation
- This notebook demonstrates how to fine-tune Qwen 2.5 (1.5B parameter model) to generate medical SOAP notes from doctor-patient dialogues.
- SOAP notes are structured medical documentation including Subjective, Objective, Assessment, and Plan sections.

# Setup Environment
  First, we'll mount Google Drive, install required packages, and set up our environment.
"""

from google.colab import drive
drive.mount('/gdrive')

# Install required packages
!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0
!pip install -qU openai==1.61.0 wandb
!pip install -qU json-repair==0.29.1
!pip install -qU faker==35.2.0
#!pip install -qU vllm==0.7.2

# Clone LLaMA-Factory
!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
!cd LLaMA-Factory && pip install -e .

"""# Authentication Setup
We'll set up authentication for Weights & Biases (for experiment tracking) and Hugging Face (for model downloading and uploading).
"""

# Login to WandB and HuggingFace
from google.colab import userdata
import wandb

wandb.login(key=userdata.get('wandb'))
hf_token = userdata.get('huggingface')
!huggingface-cli login --token {hf_token}

"""# Import Libraries
Now we'll import the essential libraries for our fine-tuning process.
"""

import json
import os
from os.path import join
import random
from tqdm.auto import tqdm
import requests

from pydantic import BaseModel, Field
from typing import List, Optional, Literal
from datetime import datetime

import json_repair

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# Set directories and model
data_dir = "/gdrive/MyDrive/qwen-finetune-v2"
base_model_id = "Qwen/Qwen2.5-1.5B-Instruct"

device = "cuda"
torch_dtype = None

"""# Data Loading and Preparation
We'll load the medical dialogue dataset and format it for fine-tuning.
"""

# Helper function to parse JSON
def parse_json(text):
    try:
        return json_repair.loads(text)
    except:
        return None

# Load dataset
from datasets import load_dataset, concatenate_datasets

dataset = load_dataset("omi-health/medical-dialogue-to-soap-summary")

# Combine train and test for simpler handling
dataset['train'] = concatenate_datasets([dataset['train'], dataset['test']])
dataset['train'] = concatenate_datasets([dataset['train'], dataset['validation']])
dataset.pop('test')
dataset.pop('validation')

print(f"Total samples in dataset: {len(dataset['train'])}")

"""# Define SOAP Summary Task
We'll use Pydantic to create a schema for our SOAP notes, which will help structure the model's output.
"""

class SOAPSummary(BaseModel):
    subjective: str = Field(...,
                           description="Patient's subjective experiences, symptoms, and concerns as reported in the dialogue.")
    objective: str = Field(...,
                          description="Objective medical findings, vital signs, and examination results mentioned in the dialogue.")
    assessment: str = Field(...,
                           description="Clinician's assessment and diagnosis of the patient's condition based on the dialogue.")
    plan: str = Field(...,
                     description="Treatment plan, medications, follow-up instructions, and recommendations discussed in the dialogue.")

"""# Prepare Training Data
Now we'll format the dataset for fine-tuning with the LLaMA-Factory framework.
"""

# Create directory if it doesn't exist
os.makedirs(join(data_dir, "datasets", "llamafactory-finetune-data"), exist_ok=True)

# Format Dataset for Fine-tuning with JSON structure
llm_finetunning_data = []

system_message = "\n".join([
    "You are a medical professional tasked with creating structured SOAP notes from patient-doctor dialogues.",
    "Generate a comprehensive SOAP summary that includes Subjective, Objective, Assessment, and Plan sections.",
    "Your summary should be concise yet comprehensive, capturing all relevant medical information.",
    "Output must be in valid JSON format according to the provided schema.",
    "Do not include any text outside the JSON structure."
])

# Process dataset
for i, entry in enumerate(tqdm(dataset['train'])):
    dialogue = entry['dialogue']
    soap_summary = entry['soap']

    components = {}

    if "S:" in soap_summary and "O:" in soap_summary:
        s_text = soap_summary.split("S:")[1].split("O:")[0].strip()
        components["subjective"] = s_text

    if "O:" in soap_summary and "A:" in soap_summary:
        o_text = soap_summary.split("O:")[1].split("A:")[0].strip()
        components["objective"] = o_text

    if "A:" in soap_summary and "P:" in soap_summary:
        a_text = soap_summary.split("A:")[1].split("P:")[0].strip()
        components["assessment"] = a_text

    if "P:" in soap_summary:
        p_text = soap_summary.split("P:")[1].strip()
        components["plan"] = p_text

    # If we couldn't parse properly, use placeholders to prevent errors
    if "subjective" not in components:
        components["subjective"] = "Information not available"
    if "objective" not in components:
        components["objective"] = "Information not available"
    if "assessment" not in components:
        components["assessment"] = "Information not available"
    if "plan" not in components:
        components["plan"] = "Information not available"

    # Create JSON output for training
    json_output = json.dumps(components, ensure_ascii=False)

    llm_finetunning_data.append({
        "system": system_message,
        "instruction": "\n".join([
            "# Patient-Doctor Dialogue:",
            dialogue,
            "",
            "# Task:",
            "Generate a structured SOAP summary from the above medical dialogue.",
            "The summary should include Subjective, Objective, Assessment, and Plan sections in JSON format.",
            "",
            "# JSON Schema:",
            json.dumps(SOAPSummary.model_json_schema(), ensure_ascii=False),
            "",
            "# SOAP Summary:",
            "```json"
        ]),
        "input": "",
        "output": json_output + "\n```",
        "history": []
    })

"""# Examine Sample Data
Let's look at a sample from our dataset to understand what we're working with.
"""

print(dataset['train'][121]['soap'])

print(llm_finetunning_data[121])

"""# Split Dataset
We'll split our dataset into training and evaluation sets.
"""

# Shuffle data with a fixed seed for reproducibility
random.Random(101).shuffle(llm_finetunning_data)

# Split into train and evaluation sets (90/10 split)
train_ratio = 0.9
train_size = int(len(llm_finetunning_data) * train_ratio)

train_ds = llm_finetunning_data[:train_size]
eval_ds = llm_finetunning_data[train_size:]

print(f"Training samples: {len(train_ds)}")
print(f"Evaluation samples: {len(eval_ds)}")

# Save datasets
with open(join(data_dir, "datasets", "llamafactory-finetune-data", "train.json"), "w", encoding="utf8") as dest:
    json.dump(train_ds, dest, ensure_ascii=False)

with open(join(data_dir, "datasets", "llamafactory-finetune-data", "val.json"), "w", encoding="utf8") as dest:
    json.dump(eval_ds, dest, ensure_ascii=False)

print(f"Datasets saved to {join(data_dir, 'datasets', 'llamafactory-finetune-data')}")

"""# Test Base Model
Before fine-tuning, let's test the base model's capabilities on our task.
"""

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    torch_dtype=torch_dtype
)

tokenizer = AutoTokenizer.from_pretrained(base_model_id)

# Test with a sample
sample_dialogue = dataset['train'][121]['dialogue']
print("Sample dialogue:\n", sample_dialogue[:300], "...\n")

# Create system message that emphasizes JSON output
system_message = "\n".join([
    "You are a medical professional tasked with creating structured SOAP notes from patient-doctor dialogues.",
    "Generate a comprehensive SOAP summary that includes Subjective, Objective, Assessment, and Plan sections.",
    "Your summary should be concise yet comprehensive, capturing all relevant medical information.",
    "Output must be in valid JSON format according to the provided schema.",
    "Do not include any text outside the JSON structure."
])

# Create testing prompt with JSON schema
sample_messages = [
    {
        "role": "system",
        "content": system_message
    },
    {
        "role": "user",
        "content": "\n".join([
            "# Patient-Doctor Dialogue:",
            sample_dialogue,
            "",
            "# Task:",
            "Generate a structured SOAP summary from the above medical dialogue.",
            "The summary should include Subjective, Objective, Assessment, and Plan sections in JSON format.",
            "",
            "# JSON Schema:",
            json.dumps(SOAPSummary.model_json_schema(), ensure_ascii=False),
            "",
            "# SOAP Summary:",
            "```json"
        ])
    }
]

# Apply chat template
text = tokenizer.apply_chat_template(
    sample_messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response
model_inputs = tokenizer([text], return_tensors="pt", padding=True).to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=1024,
    do_sample=False,
    temperature=0.1,
    repetition_penalty=1.1
)

generated_ids = [
    output_ids[len(input_ids):]
    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print("Model response preview:")
print(response)

"""# Validate Model Output
Now we'll check if the model's output conforms to our JSON schema.
"""

print("\nAttempting to parse JSON response:")
try:
    # Find JSON block if the model wrapped it with ```json and ```
    if "```json" in response and "```" in response.split("```json", 1)[1]:
        json_str = response.split("```json", 1)[1].split("```", 1)[0].strip()
    else:
        # If no code blocks, try to parse the whole response
        json_str = response.strip()

    # Clean up any trailing commas that might cause parsing errors
    json_str = json_str.replace(",\n}", "\n}")
    json_str = json_str.replace(",\n]", "\n]")

    parsed_json = json_repair.loads(json_str)
    print("Successfully parsed JSON:")
    print(json.dumps(parsed_json, indent=2))

    try:
        soap_summary = SOAPSummary(**parsed_json)
        print("\nValidation successful! All required fields are present.")
    except Exception as e:
        print(f"\nValidation error: {e}")
        print("JSON does not match the expected schema.")

except Exception as e:
    print(f"Failed to parse JSON: {e}")
    print("Raw response may not contain valid JSON.")

# Add a function to properly format SOAP
def format_soap_display(soap_dict):
    """Format SOAP dictionary into readable text format"""
    formatted = []
    if "subjective" in soap_dict:
        formatted.append(f"S: {soap_dict['subjective']}")
    if "objective" in soap_dict:
        formatted.append(f"O: {soap_dict['objective']}")
    if "assessment" in soap_dict:
        formatted.append(f"A: {soap_dict['assessment']}")
    if "plan" in soap_dict:
        formatted.append(f"P: {soap_dict['plan']}")
    return "\n\n".join(formatted)

# If we successfully parsed JSON, display it in the traditional SOAP format
if 'parsed_json' in locals():
    print("\nFormatted SOAP Summary:")
    print(format_soap_display(parsed_json))

# Compare with ground truth
print("\nGround truth SOAP summary:")
print(dataset['train'][121]['soap'])

"""# Configure LLaMA-Factory
We'll set up LLaMA-Factory to recognize our custom datasets.
"""

dataset_info = {
    "soap_finetune_train": {
        "file_name": f"{data_dir}/datasets/llamafactory-finetune-data/train.json",
        "columns": {
            "prompt": "instruction",
            "query": "input",
            "response": "output",
            "system": "system",
            "history": "history"
        }
    },
    "soap_finetune_val": {
        "file_name": f"{data_dir}/datasets/llamafactory-finetune-data/val.json",
        "columns": {
            "prompt": "instruction",
            "query": "input",
            "response": "output",
            "system": "system",
            "history": "history"
        }
    }
}

# Write the dataset configuration to a JSON file
with open("/content/LLaMA-Factory/data/dataset_info.json", "r") as f:
    existing_info = json.load(f)

# Update with our new entries
existing_info.update(dataset_info)

# Write back the updated configuration
with open("/content/LLaMA-Factory/data/dataset_info.json", "w") as f:
    json.dump(existing_info, f, indent=4)

print("Dataset configuration updated successfully!")

"""# LLaMA Factory Fine-tuning Configuration
We'll create a configuration file for LLaMA-Factory that specifies our fine-tuning settings.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/LLaMA-Factory/examples/train_lora/soap_finetune.yaml
# 
# ### model
# model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
# trust_remote_code: true
# 
# ### method
# stage: sft
# do_train: true
# finetuning_type: lora
# lora_rank: 16
# lora_alpha: 32      # Standard setting (2x rank)
# lora_dropout: 0.05
# lora_target: all
# 
# ### dataset
# dataset: soap_finetune_train
# eval_dataset: soap_finetune_val
# template: qwen
# cutoff_len: 2048
# overwrite_cache: true
# preprocessing_num_workers: 4
# 
# ### output
# output_dir: /gdrive/MyDrive/qwen-finetune-v2/models/
# logging_steps: 50
# save_steps: 200
# plot_loss: true
# 
# ### train
# per_device_train_batch_size: 1  # Minimum batch size for T4
# gradient_accumulation_steps: 16
# learning_rate: 5.0e-5
# num_train_epochs: 1.5
# lr_scheduler_type: cosine
# warmup_ratio: 0.05
# fp16: true
# gradient_checkpointing: true  # Enable gradient checkpointing to save memory
# max_grad_norm: 0.3
# ddp_timeout: 180000000
# bf16: false
# 
# ### eval
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 200
# max_new_tokens: 1024
# 
# ### reports
# report_to: wandb
# run_name: qwen-finetune-v2
# 
# # Hub upload configuration
# push_to_hub: true
# export_hub_model_id: "hazem74/soap-summary-qwen-v2"
# hub_private_repo: true
# hub_strategy: checkpoint

"""# Start Fine-tuning
Now we'll start the fine-tuning process using LLaMA-Factory.
"""

!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/soap_finetune.yaml

"""# LLaMA Factory finetuning Configuration (continue from checkpoint-400)
If training was interrupted or you want to continue from a checkpoint, you can use this configuration:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/LLaMA-Factory/examples/train_lora/soap_finetune.yaml
# 
# ### model
# model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
# trust_remote_code: true
# 
# ### method
# stage: sft
# do_train: true
# finetuning_type: lora
# lora_rank: 16
# lora_alpha: 32      # Standard setting (2x rank)
# lora_dropout: 0.05
# lora_target: all
# 
# ### checkpoint to resume from
# resume_from_checkpoint: /gdrive/MyDrive/qwen-finetune-v2/models/checkpoint-400
# 
# ### dataset
# dataset: soap_finetune_train
# eval_dataset: soap_finetune_val
# template: qwen
# cutoff_len: 2048
# overwrite_cache: true
# preprocessing_num_workers: 4
# 
# ### output
# output_dir: /gdrive/MyDrive/qwen-finetune-v2/models/
# logging_steps: 50
# save_steps: 200
# plot_loss: true
# 
# ### train
# per_device_train_batch_size: 1  # Minimum batch size for T4
# gradient_accumulation_steps: 16
# learning_rate: 5.0e-5
# num_train_epochs: 1.5
# lr_scheduler_type: cosine
# warmup_ratio: 0.05
# fp16: true
# gradient_checkpointing: true  # Enable gradient checkpointing to save memory
# max_grad_norm: 0.3
# ddp_timeout: 180000000
# bf16: false
# 
# ### eval
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 200
# max_new_tokens: 1024
# 
# ### reports
# report_to: wandb
# run_name: qwen-finetune-v2
# 
# # Hub upload configuration
# push_to_hub: true
# export_hub_model_id: "hazem74/soap-summary-qwen-v2"
# hub_private_repo: true
# hub_strategy: checkpoint

!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/soap_finetune.yaml

"""# LLaMA Factory finetuning Configuration (continue from checkpoint-600)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/LLaMA-Factory/examples/train_lora/soap_finetune.yaml
# 
# ### model
# model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
# trust_remote_code: true
# 
# ### method
# stage: sft
# do_train: true
# finetuning_type: lora
# lora_rank: 16
# lora_alpha: 32      # Standard setting (2x rank)
# lora_dropout: 0.05
# lora_target: all
# 
# ### checkpoint to resume from
# resume_from_checkpoint: /gdrive/MyDrive/qwen-finetune-v2/models/checkpoint-600
# 
# ### dataset
# dataset: soap_finetune_train
# eval_dataset: soap_finetune_val
# template: qwen
# cutoff_len: 2048
# overwrite_cache: true
# preprocessing_num_workers: 4
# 
# ### output
# output_dir: /gdrive/MyDrive/qwen-finetune-v2/models/
# logging_steps: 50
# save_steps: 200
# plot_loss: true
# 
# ### train
# per_device_train_batch_size: 1  # Minimum batch size for T4
# gradient_accumulation_steps: 16
# learning_rate: 5.0e-5
# num_train_epochs: 1.5
# lr_scheduler_type: cosine
# warmup_ratio: 0.05
# fp16: true
# gradient_checkpointing: true
# max_grad_norm: 0.3
# ddp_timeout: 180000000
# bf16: false
# 
# ### eval
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 200
# max_new_tokens: 1024
# 
# ### reports
# report_to: wandb
# run_name: qwen-finetune-v2
# 
# # Hub upload configuration
# push_to_hub: true
# export_hub_model_id: "hazem74/soap-summary-qwen-v2"
# hub_private_repo: true
# hub_strategy: checkpoint

!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/soap_finetune.yaml

"""# Testing the Fine-tuned Model
Now let's test our fine-tuned model on some examples.
"""

!pip install -U bitsandbytes

import os
import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Set up offload directory
offload_dir = "/tmp/offload_dir"
os.makedirs(offload_dir, exist_ok=True)

# Specify the checkpoint path
adapter_path = "/gdrive/MyDrive/qwen-finetune-v2/models/checkpoint-843"
print(f"Using adapter from checkpoint: {adapter_path}")

# Base model ID
base_model_id = "Qwen/Qwen2.5-1.5B-Instruct"
print(f"Using base model: {base_model_id}")

# Setup quantization config to reduce memory usage
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# Load base model with quantization
model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="auto",
    offload_folder=offload_dir,
    torch_dtype=torch.float16
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_id)

# Load adapter weights
model = PeftModel.from_pretrained(model, adapter_path)

print("Model loaded successfully!")

"""# Create Helper Function for SOAP Generation
We'll create a utility function to generate SOAP notes with our fine-tuned model.
"""

def generate_soap_summary(dialogue, max_tokens=2048):
    messages = [
    {
        "role": "system",
        "content": system_message
    },
    {
        "role": "user",
        "content": "\n".join([
            "# Patient-Doctor Dialogue:",
            dialogue,
            "",
            "# Task:",
            "Generate a structured SOAP summary from the above medical dialogue.",
            "The summary should include Subjective, Objective, Assessment, and Plan sections in JSON format.",
            "",
            "# JSON Schema:",
            json.dumps(SOAPSummary.model_json_schema(), ensure_ascii=False),
            "",
            "# SOAP Summary:",
            "```json"
        ])
    }
]

    # Apply prompt formatting
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt", padding=True).to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=max_tokens,
            do_sample=False
        )

    # Trim prompt from output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]



dialogue = dataset['train'][508]['dialogue']
# Print a short preview of the dialogue
print(f"Dialogue preview: {dialogue}...\n")

# Generate and print the summary
summary = generate_soap_summary(dialogue)
print("Generated SOAP Summary:")
print(summary)
print("-" * 80)

"""# Cost Estimation for Inference
Let's estimate the computational cost of running inference with our model.
"""

from tqdm.auto import tqdm
import random
from faker import Faker
from datetime import datetime
import torch

# Initialize Faker
fake = Faker()
start_time = datetime.now()

# Start tracking tokens
input_tokens = 0
output_tokens = 0

num_samples = 30

for i in tqdm(range(num_samples)):
    # Generate random text using Faker
    dialogue = fake.text(max_nb_chars=random.randint(400, 800))

    # Create the messages
    messages = [
    {
        "role": "system",
        "content": system_message
    },
    {
        "role": "user",
        "content": "\n".join([
            "# Patient-Doctor Dialogue:",
            dialogue,
            "",
            "# Task:",
            "Generate a structured SOAP summary from the above medical dialogue.",
            "The summary should include Subjective, Objective, Assessment, and Plan sections in JSON format.",
            "",
            "# JSON Schema:",
            json.dumps(SOAPSummary.model_json_schema(), ensure_ascii=False),
            "",
            "# SOAP Summary:",
            "```json"
        ])
    }
]

    # Get the input token count
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    current_input_tokens = len(tokenizer.encode(input_text))
    input_tokens += current_input_tokens

    # Generate the response
    response = generate_soap_summary(dialogue)

    # Get the output token count
    current_output_tokens = len(tokenizer.encode(response))
    output_tokens += current_output_tokens

    if i < 3:  # Just show stats for first few samples
        print(f"\nSample {i+1}:")
        print(f"  Random text length (chars): {len(dialogue)}")
        print(f"  Input tokens: {current_input_tokens}")
        print(f"  Output tokens: {current_output_tokens}")
        print(f"  Ratio (output/input): {current_output_tokens/current_input_tokens:.2f}")

# Calculate totals
total_time = (datetime.now() - start_time).total_seconds()
total_tokens = input_tokens + output_tokens

print(f"\n--- Cost Estimation Summary ---")
print(f"Total Processing Time: {total_time:.2f} seconds")
print(f"Average Time Per Sample: {total_time/num_samples:.2f} seconds")
print(f"Input Tokens: {input_tokens} (Average: {input_tokens/num_samples:.1f} per sample)")
print(f"Output Tokens: {output_tokens} (Average: {output_tokens/num_samples:.1f} per sample)")
print(f"Total Tokens: {total_tokens} (Average: {total_tokens/num_samples:.1f} per sample)")
print(f"Output/Input Token Ratio: {output_tokens/input_tokens:.2f}")

# Estimated cost calculation
input_cost_per_1k = 0.0015  # Example rate: $0.0015 per 1K input tokens
output_cost_per_1k = 0.002  # Example rate: $0.002 per 1K output tokens

estimated_input_cost = (input_tokens / 1000) * input_cost_per_1k
estimated_output_cost = (output_tokens / 1000) * output_cost_per_1k
total_estimated_cost = estimated_input_cost + estimated_output_cost

print(f"\n--- Estimated Costs (using example rates) ---")
print(f"Input Cost: ${estimated_input_cost:.4f}")
print(f"Output Cost: ${estimated_output_cost:.4f}")
print(f"Total Estimated Cost: ${total_estimated_cost:.4f}")
print(f"Cost Per Sample: ${total_estimated_cost/num_samples:.4f}")

print("No. of Tokens per second = "+str(total_tokens/total_time))

"""# Pushing Model to **HuggingFace** Hub
Now we'll push our fine-tuned model to the Hugging Face Hub for easy sharing and deployment.
"""

!pip install -q peft huggingface_hub

# Log in to Hugging Face
from huggingface_hub import login
from google.colab import userdata

hf_token = userdata.get('huggingface')
login(token=hf_token)

# Load the base model
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Define model paths
base_model_id = "Qwen/Qwen2.5-1.5B-Instruct"
print(f"Loading base model: {base_model_id}")

# Load the model
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    device_map="auto"
)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load the adapter
from peft import PeftModel

# Path to your fine-tuned adapter
adapter_path = "/gdrive/MyDrive/qwen-finetune-v2/models/checkpoint-843"
print(f"Loading adapter from: {adapter_path}")

# Load the adapter onto the base model
model = PeftModel.from_pretrained(base_model, adapter_path)

# Merge the adapter weights with the base model
print("Merging adapter with base model...")
merged_model = model.merge_and_unload()

# Create a model card
import os

# Your Hugging Face username and model name
your_username = "hazem74"
model_name = "qwen-soap-summary-v2"
hf_model_repo = f"{your_username}/{model_name}"

# Create a model card
model_card = f"""
---
language:
- en
tags:
- medical
- healthcare
- SOAP notes
- clinical documentation
license: mit
datasets:
- omi-health/medical-dialogue-to-soap-summary
---

# DeepSeek SOAP Summary Generator

This model is fine-tuned to generate SOAP (Subjective, Objective, Assessment, Plan) summaries from patient-doctor dialogues.

## Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained("{hf_model_repo}", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("{hf_model_repo}")

# Sample dialogue
dialogue = \"\"\"
Doctor: Hello, how are you feeling today?
Patient: I've been having some chest pain for the last two days.
Doctor: Can you describe the pain?
Patient: It's a sharp pain, mostly on the left side.
\"\"\"

# Format the prompt
system_message = "You are a medical professional tasked with creating SOAP notes from patient-doctor dialogues."
user_content = f\"\"\"
# Patient-Doctor Dialogue:
{{dialogue}}

# Task:
Generate a SOAP summary from the above medical dialogue.
The summary should include Subjective, Objective, Assessment, and Plan sections.

# SOAP Summary:
\"\"\"

messages = [
    {{"role": "system", "content": system_message}},
    {{"role": "user", "content": user_content}}
]

# Generate SOAP summary
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    inputs.input_ids,
    max_new_tokens=512,
    temperature=0.3,
    top_p=0.9
)

soap_summary = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
print(soap_summary)```

Limitations
This model assists healthcare professionals but should not replace human judgment. Always review generated summaries for accuracy.
"""

# Create a temporary directory and save the model
temp_dir = "/tmp/merged_model"
os.makedirs(temp_dir, exist_ok=True)

print(f"Saving merged model to {temp_dir}")
merged_model.save_pretrained(temp_dir, safe_serialization=True)
tokenizer.save_pretrained(temp_dir)

# Save the model card
with open(os.path.join(temp_dir, "README.md"), "w") as f:
    f.write(model_card)

# Push the model to Hugging Face Hub
from huggingface_hub import HfApi

api = HfApi()
print(f"Pushing model to Hugging Face Hub: {hf_model_repo}")

# Create the repository if it doesn't exist
try:
    api.create_repo(repo_id=hf_model_repo, private=True, exist_ok=True)
    print(f"Repository {hf_model_repo} is ready")
except Exception as e:
    print(f"Note: {e}")

# Upload all files
print("Uploading files...")
api.upload_folder(
    folder_path=temp_dir,
    repo_id=hf_model_repo,
    token=hf_token
)

"""# Test the Model from HuggingFace
Now we will test the Model after uploading it to HuggingFace hub
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load once and reuse
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(hf_model_repo)
model = AutoModelForCausalLM.from_pretrained(
    hf_model_repo,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True
)

# System message (keep it same as before)
system_message = "\n".join([
    "You are a medical professional tasked with creating structured SOAP notes from patient-doctor dialogues.",
    "Generate a comprehensive SOAP summary that includes Subjective, Objective, Assessment, and Plan sections.",
    "Your summary should be concise yet comprehensive, capturing all relevant medical information.",
    "Output must be in valid JSON format according to the provided schema.",
    "Do not include any text outside the JSON structure."
])

# Test dialogue
test_dialogue = """
Doctor: Good morning. What brings you in today?
Patient: Hi, Doctor. I’ve been having a persistent cough for about two weeks now. It gets worse at night, and sometimes I feel short of breath.
Doctor: I see. Any fever, chills, or chest pain?
Patient: No fever or chills, but I do get some mild chest discomfort when I cough a lot.
Doctor: Do you have a history of asthma or any allergies?
Patient: I have mild asthma, but it’s usually well-controlled. I use an inhaler only occasionally.
Doctor: Any recent exposure to someone who was sick?
Patient: Yes, my daughter had a cold last week.
Doctor: Have you noticed any wheezing or phlegm?
Patient: Yes, there’s some wheezing, especially at night, and I’ve been coughing up clear phlegm.
Doctor: How’s your appetite and energy level?
Patient: Appetite is okay, but I’ve been feeling a bit tired lately.
Doctor: Alright. I’ll listen to your lungs now... (examines patient) You have some mild wheezing in both lungs, more on the right side.
Patient: Is it serious?
Doctor: It sounds like an asthma flare-up possibly triggered by a viral infection. Your oxygen levels are normal, and there’s no sign of pneumonia.
Patient: Okay, what should I do?
Doctor: I’ll prescribe a short course of oral steroids and advise using your inhaler regularly for the next few days. Drink plenty of fluids and rest. If your breathing worsens, go to the ER immediately."""

sample_dialogue = dataset['train'][121]['dialogue']

summary = generate_soap_summary(test_dialogue)
#print(dataset['train'][121]['soap'])
print(summary)

